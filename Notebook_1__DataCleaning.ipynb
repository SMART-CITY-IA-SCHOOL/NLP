{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSZc9AYDMJ2D"
   },
   "source": [
    "![title](ia_school_report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h1>Notebook 1: Import des fichiers et Features Engineering</h1>\n",
    "    <p>Notre projet consiste à analyser des données d'une application de Smart-City de l'entreprise Losinger Matarazzi.\n",
    "Dans ce premier notebook nous allons importer nos jeux de données situés dans un Blob Azure Storage. Nous allons ensuite chercher à rendre les données exploitables facilement en transformant les dataset. L'exploitation du dataset et les insights que nous en tirons sont exposés dans le Notebook 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cEubigKISlV",
    "outputId": "02d67050-0c3a-4cad-aa65-5143625767d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage-blob\n",
      "  Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 11.2 MB/s \n",
      "\u001b[?25hCollecting msrest>=0.6.21\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 1.3 MB/s \n",
      "\u001b[?25hCollecting cryptography>=2.1.4\n",
      "  Downloading cryptography-37.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 28.9 MB/s \n",
      "\u001b[?25hCollecting azure-core<2.0.0,>=1.15.0\n",
      "  Downloading azure_core-1.23.1-py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 23.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.15.0->azure-storage-blob) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.15.0->azure-storage-blob) (2.23.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.15.0->azure-storage-blob) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 359 kB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-storage-blob) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-storage-blob) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.15.0->azure-storage-blob) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.15.0->azure-storage-blob) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.15.0->azure-storage-blob) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-storage-blob) (3.2.0)\n",
      "Installing collected packages: isodate, msrest, cryptography, azure-core, azure-storage-blob\n",
      "Successfully installed azure-core-1.23.1 azure-storage-blob-12.11.0 cryptography-37.0.1 isodate-0.6.1 msrest-0.6.21\n"
     ]
    }
   ],
   "source": [
    "pip install azure-storage-blob --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIQ-0INIIqt2",
    "outputId": "25d32852-6ed7-43e1-c04c-11c08c546616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tokens\n",
      "  Downloading tokens-0.0.3.tar.gz (3.8 kB)\n",
      "Building wheels for collected packages: Tokens\n",
      "  Building wheel for Tokens (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Tokens: filename=tokens-0.0.3-py3-none-any.whl size=4232 sha256=75b158656edd45dbccf5a43fd8b379084a700dd1faf722d90fd03ce409479f25\n",
      "  Stored in directory: /root/.cache/pip/wheels/3c/72/56/526e5c2a15bf7576f90fbe72e2d956da7cedbeb052552e7f5b\n",
      "Successfully built Tokens\n",
      "Installing collected packages: Tokens\n",
      "Successfully installed Tokens-0.0.3\n"
     ]
    }
   ],
   "source": [
    "pip install Tokens --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRYgrmPUX_8I",
    "outputId": "fdba17d2-7e43-4bac-8cf4-7fecf643e9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.10.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 3.9 MB/s \n",
      "\u001b[?25hCollecting hdbscan>=0.8.28\n",
      "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.2 MB 28.9 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 7.0 MB/s \n",
      "\u001b[?25hCollecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (4.64.0)\n",
      "Requirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (3.13)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.21.6)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.3.5)\n",
      "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (1.4.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 50.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.12.0+cu113)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.2.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 52.7 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.11.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 50.7 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 58.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Collecting pyyaml<6.0\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 49.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2019.12.20)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.8)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 48.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent, sacremoses\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2330794 sha256=5143e846daf0ad5d864e4c52ed82e17b1625d540c86a257071f2066a3a8e876b\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=b0515844a98dcc4c0bd65a82a3051e7180d43b28933e2e41e8a9352347d38037\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=e9427f7c56304d13b5465f255f5f3c27e92efeb6b63747d57358fad9694cf36a\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=628c932a1a4680576004672734fa9c32b20c00445c1bce418f8a9fcb089c1afb\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=3699f01141449b110acbee50a28009fa45529e73e31a857ec5ed1999c5328fb6\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built hdbscan sentence-transformers umap-learn pynndescent sacremoses\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, pynndescent, umap-learn, sentence-transformers, hdbscan, bertopic\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed bertopic-0.10.0 hdbscan-0.8.28 huggingface-hub-0.5.1 pynndescent-0.5.6 pyyaml-5.4.1 sacremoses-0.0.53 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91_ul_dfMT2t"
   },
   "source": [
    "# Load Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "938Tv-KeIW00",
    "outputId": "15905e6b-391f-4d8f-f8b5-fd502898f2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Blob Storage v12.11.0 - installed\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import os, uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__,generate_account_sas, ResourceTypes, AccountSasPermissions\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import spacy\n",
    "NER = spacy.load('en_core_web_sm')\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "import datetime\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    print(\"Azure Blob Storage v\" + __version__ + \" - installed\")\n",
    "\n",
    "except Exception as ex:\n",
    "    print('Exception:')\n",
    "    print(ex)\n",
    "    print ('Execute ''pip install azure-storage-blob'' to install the package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0qBAiSjMkCt"
   },
   "source": [
    "# Config BlobStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXOTrt-0IW3H",
    "outputId": "40f007d6-5f9e-42ac-d884-b7a394dde4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bycnitstdatafactlosinger.blob.core.windows.net/iaschool-allthings?sv=2020-10-02&st=2022-02-25T08%3A54%3A54Z&se=2022-05-20T22%3A00%3A00Z&sr=c&sp=racwdxl&sig=uHFg6h38jIkKY4UzEalvnRPTlXOISweKQTHcxxOHBnk%3D\n",
      "anonymized_posts/anonymized_dataset_Allthings_Posts.csv\n",
      "\n",
      "anonymized_posts/entity_recognition_dataset_Allthings_Posts.csv\n",
      "\n",
      "anonymized_tickets/anonymized_dataset_Allthings_Tickets.csv\n",
      "\n",
      "anonymized_tickets/entity_recognition_dataset_Allthings_Tickets.csv\n",
      "\n",
      "post_events/ALLTHINGS_post_events.csv\n",
      "\n",
      "sample_data/POSTS_Echantillon traduit et anonymisé.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_name='bycnitstdatafactlosinger'\n",
    "account_url = 'https://'+account_name+'.blob.core.windows.net/'\n",
    "container_name = 'iaschool-allthings'\n",
    "shared_access_signature = '?sv=2020-10-02&st=2022-02-25T08%3A54%3A54Z&se=2022-05-20T22%3A00%3A00Z&sr=c&sp=racwdxl&sig=uHFg6h38jIkKY4UzEalvnRPTlXOISweKQTHcxxOHBnk%3D'\n",
    "connection_string = account_url+container_name+shared_access_signature\n",
    "print(connection_string)\n",
    "\n",
    "# Create the BlobServiceClient object \n",
    "blob_service_client = BlobServiceClient(account_url=account_url, credential=shared_access_signature)\n",
    "\n",
    "# Create the ContainerClient object \n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "blob_files = container_client.list_blobs()\n",
    "for blob in blob_files:\n",
    "    print(blob.name + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOdtW_RZMoaF"
   },
   "source": [
    "# Load Data from BlobStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y-fVM-nIW_G"
   },
   "outputs": [],
   "source": [
    "# Define temp file\n",
    "posts = 'anonymized_dataset_Allthings_Posts.csv'\n",
    "post_event = \"ALLTHINGS_post_events.csv\"\n",
    "post1 = '/entity_recognition_dataset_Allthings_Posts.csv'\n",
    "tickets = 'anonymized_dataset_Allthings_Tickets.csv'\n",
    "tickets1 = 'entity_recognition_dataset_Allthings_Tickets.csv'\n",
    "sample_data = 'POSTS_Echantillon traduit et anonymisé.csv'\n",
    "\n",
    "# Download data into the temp file\n",
    "with open(posts, \"wb\") as my_blob:\n",
    "    blob_data = container_client.download_blob(\"anonymized_posts/anonymized_dataset_Allthings_Posts.csv\")\n",
    "    blob_data.readinto(my_blob)\n",
    "    \n",
    "df_post = pd.read_csv(posts, sep=';',encoding='utf-8')\n",
    "os.remove(posts)\n",
    "\n",
    "df_post = df_post.rename(columns = {\"User_ID\":\"ID_USER\", \"Post_ID\":\"ID_POST\"})\n",
    "df_post['HEURE'] = df_post['DateTime'].apply(lambda x : x[11:19])\n",
    "\n",
    "df_post.DateTime = pd.to_datetime(df_post.DateTime)\n",
    "df_post['DateTime'] = df_post['DateTime'].dt.date\n",
    "df_post = df_post[[\"ID_POST\", \"DateTime\", \"HEURE\", \"Type\", \"Content\"]]\n",
    "df_post = df_post.dropna()\n",
    "df_post = df_post.reset_index()\n",
    "df_post.Content = df_post.Content.apply(lambda x : re.sub(r\"\\<.*?\\>\", \"\", x))\n",
    "\n",
    "\n",
    "# Download data into the temp file\n",
    "with open(post_event, \"wb\") as my_blob:\n",
    "    blob_data = container_client.download_blob(\"post_events/ALLTHINGS_post_events.csv\")\n",
    "    blob_data.readinto(my_blob)\n",
    "    \n",
    "df_post_event = pd.read_csv(post_event, sep=',',encoding='utf-8')\n",
    "os.remove(post_event)\n",
    "\n",
    "df_post_event.EVENT_DATETIME = pd.to_datetime(df_post_event.EVENT_DATETIME)\n",
    "df_post_event['EVENT_DATETIME'] = df_post_event['EVENT_DATETIME'].dt.tz_localize(None)\n",
    "\n",
    "df_post_event['EVENT_DATETIME'] = df_post_event['EVENT_DATETIME'].dt.date\n",
    "df_post_event = df_post_event[[\"EVENT_DATETIME\",\"ID_POST\", \"LB_EVENT\"]]\n",
    "\n",
    "event = df_post_event.groupby([\"ID_POST\",\"LB_EVENT\"]).agg({\"LB_EVENT\" : \"count\", \"EVENT_DATETIME\" : \"first\"})\n",
    "\n",
    "event.columns = [\"NBR\", \"DATE\"]\n",
    "event = event.reset_index()\n",
    "\n",
    "# Download data into the temp file\n",
    "with open(post1, \"wb\") as my_blob:\n",
    "    blob_data = container_client.download_blob(\"anonymized_posts/entity_recognition_dataset_Allthings_Posts.csv\")\n",
    "    blob_data.readinto(my_blob)\n",
    "df_post1 = pd.read_csv(post1, sep=';',encoding='utf-8')\n",
    "os.remove(post1)\n",
    "\n",
    "# Download data into the temp file\n",
    "with open(tickets, \"wb\") as my_blob:\n",
    "    blob_data = container_client.download_blob(\"anonymized_tickets/anonymized_dataset_Allthings_Tickets.csv\")\n",
    "    blob_data.readinto(my_blob)\n",
    "df_tickets = pd.read_csv(tickets, sep=';',encoding='utf-8')\n",
    "os.remove(tickets)\n",
    "\n",
    "# Download data into the temp file\n",
    "with open(tickets1, \"wb\") as my_blob:\n",
    "    blob_data = container_client.download_blob(\"anonymized_tickets/entity_recognition_dataset_Allthings_Tickets.csv\")\n",
    "    blob_data.readinto(my_blob)\n",
    "df_tickets1 = pd.read_csv(tickets1, sep=';',encoding='utf-8')\n",
    "os.remove(tickets1)\n",
    "\n",
    "# Download data into the temp file\n",
    "with open(sample_data, \"wb\") as my_blob:\n",
    "    blob_data = container_client.download_blob(\"sample_data/POSTS_Echantillon traduit et anonymisé.csv\")\n",
    "    blob_data.readinto(my_blob)\n",
    "df_samp_data = pd.read_csv(sample_data, sep=';',encoding='utf-8')\n",
    "os.remove(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2qzunNgEoAp"
   },
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Uc-Q61bEqjp"
   },
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "#consolidate data with previous value\n",
    "def consolidatedf_aft(df):\n",
    "    for i in range(1,len(df),1) :\n",
    "        if df.index.values[i] == df.index.values[i-1]:\n",
    "            for j in range(len(df.columns)):\n",
    "                if isNaN(df.iloc[i,j]):# or np.isnan(df.iloc[i,j]):\n",
    "                    df.iloc[i, j] = df.iloc[i-1, j]\n",
    "    PivotedEnriched = df\n",
    "    return PivotedEnriched\n",
    "\n",
    "def consolidatedf_bef(df):\n",
    "    for i in range(0,len(df)-1,1) :\n",
    "        if df.index.values[i] == df.index.values[i+1]:\n",
    "            for j in range(0,len(df.columns)):\n",
    "                if isNaN(df.iloc[i,j]) or df.iloc[i,j] == '0':#  np.isnan(df.iloc[i,j]):\n",
    "                    df.iloc[i, j] = df.iloc[i+1, j]\n",
    "    PivotedEnriched = df\n",
    "    return PivotedEnriched\n",
    "def consolidatedf(df):\n",
    "    consolidatedf_aft(df)\n",
    "    consolidatedf_bef(df)\n",
    "\n",
    "\n",
    "def avg_datetime(series):\n",
    "    dt_min = series.min()\n",
    "    deltas = [x-dt_min for x in series]\n",
    "    return dt_min + functools.reduce(operator.add, deltas) / len(deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dLuSBxsM6Gg"
   },
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uol6gVRPJwt2"
   },
   "source": [
    "## Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6izh6lBGZJl"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2>💾 DataConsolidation</h2>\n",
    "<p> On pivot le dataframe Event post pour avoir le nombre de commentaire, like et view par post. Puis on vient merger les dataset Event type post et post sur l'ID du post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1QHftzKSwC0",
    "outputId": "33c75a96-9f9f-499f-c64e-77172a8bc970"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
     ]
    }
   ],
   "source": [
    "df_post_event_merge = df_post.merge(event, on = 'ID_POST', how = 'inner')\n",
    "\n",
    "df_post_event_merge['Type'] = df_post_event_merge['Type'].str.lower()\n",
    "df_post_event_merge['LB_EVENT'] = df_post_event_merge['LB_EVENT'].str.lower()\n",
    "\n",
    "df_temp = df_post_event_merge[df_post_event_merge['LB_EVENT'].str.contains('page.viewed') | df_post_event_merge['LB_EVENT'].str.contains('post.like.created') | df_post_event_merge['LB_EVENT'].str.contains('post.like.deleted')]\n",
    "df_temps = df_temp.pivot(index = ['index', 'ID_POST'], columns = 'LB_EVENT', values = 'NBR').fillna(0).reset_index().drop(columns = ['index']).drop_duplicates()\n",
    "\n",
    "df_temps1 = df_post_event_merge[df_post_event_merge['LB_EVENT'] == df_post_event_merge['Type']].pivot(index = ['index', 'ID_POST', 'DATE'], columns = 'LB_EVENT', values = 'NBR').reset_index().set_index('ID_POST').drop(columns = 'index')\n",
    "consolidatedf(df_temps1)\n",
    "df_temps1 = df_temps1.reset_index().fillna(0).drop_duplicates(subset = ['ID_POST'])\n",
    "\n",
    "df_event_final = df_temps1.merge(df_temps, on = 'ID_POST', how = 'outer').fillna(0)#.merge(df_post_event_merge, on = 'ID_POST').drop(columns = ['HEURE', 'LB_EVENT', 'NBR', 'DateTime'])\n",
    "df_event_final['nb_like'] = df_event_final['post.like.created'] + df_event_final['post.like.deleted']\n",
    "df_event_final = df_event_final.rename(columns = {'comment':'nb_comment', 'post':'nb_post', 'page.viewed':'nb_view'}).drop(columns = ['post.like.created', 'post.like.deleted'])\n",
    "\n",
    "df_temps3 = df_post_event_merge[df_post_event_merge['LB_EVENT'] == df_post_event_merge['Type']].pivot(index = ['index', 'ID_POST'], columns = 'Type', values = 'Content').reset_index().set_index('ID_POST')\n",
    "consolidatedf(df_temps3)\n",
    "df_temps3 = df_temps3.reset_index().drop_duplicates(subset = ['comment'])\n",
    "\n",
    "df_event_final1 = df_event_final.merge(df_temps3, on = 'ID_POST')\n",
    "\n",
    "df_event_final1['comment'] = df_event_final1['comment'].str.replace('.','', 1)\n",
    "df_event_final1['post'] = df_event_final1['post'].str.replace('.','', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv_aFwxpGS-G"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<h3>🧑‍ Named Entity Recognition (NER) </h3>\n",
    "<p> La reconnaissance d'entité nous permettra de récuper la localisation du post </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJKnIC9Uc2rl"
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(len(df_event_final1['ID_POST'].unique())):\n",
    "  doc = NER(str(remove_stopwords(df_event_final1['post'][i])))\n",
    "  id_post = df_event_final1['ID_POST'][i]\n",
    "  for i in doc.ents:\n",
    "    txt = i.text\n",
    "    label = i.label_\n",
    "    l.append((id_post, txt,label))\n",
    "df_ner = pd.DataFrame(l, columns = ['ID_POST','Location', 'NER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58X_DjWBF-lG"
   },
   "outputs": [],
   "source": [
    "df_ner = df_ner[df_ner['NER'] == 'LOC'][['ID_POST', 'Location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VE57MSkdHi4i"
   },
   "outputs": [],
   "source": [
    "df_post_final = df_event_final1.merge(df_ner, on = 'ID_POST', how = 'outer').drop_duplicates().drop(columns = ['index'])\n",
    "df_post_final['Location'] = df_post_final['Location'].fillna('No Location')\n",
    "df_post_final['comment'] = df_post_final['comment'].fillna('No Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlSb3AiiLbfv",
    "outputId": "4ffccced-b35c-4f91-da83-88bf14dd8593"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-420671ed-220b-4ae5-984e-976ae2a9d236\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_POST</th>\n",
       "      <th>DATE</th>\n",
       "      <th>nb_comment</th>\n",
       "      <th>nb_post</th>\n",
       "      <th>nb_view</th>\n",
       "      <th>nb_like</th>\n",
       "      <th>comment</th>\n",
       "      <th>post</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55002b8b828463a4178b457b</td>\n",
       "      <td>2015-03-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hello, urgent service messages do not arrive...</td>\n",
       "      <td>The  team is the contact point for all questio...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5508260e828463ae4a8b45fa</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No Comment</td>\n",
       "      <td>How is a good neighbourhood created? Send th...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552547e8828463e9288b4586</td>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maybe take a look at the old apartment.</td>\n",
       "      <td>Dear , :  our cat is missing. The cat's name...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552f62c78284636b568b4577</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Good and original thing :-) Great and useful...</td>\n",
       "      <td>we want to know! Your opinion!  in the Erle...</td>\n",
       "      <td>Erlenmatt West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552fa7218284634b608b4581</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you very much for this hint! We will b...</td>\n",
       "      <td>We from the  have the Erlenmattpark right in...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7769</th>\n",
       "      <td>6196673209d8f566a8033e74</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>With us exactly the same, we had to take a c...</td>\n",
       "      <td>again no hot water  ( in the 35 )  moderate...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7770</th>\n",
       "      <td>6196673209d8f566a8033e74</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes  OK again, is already tedious? And unfor...</td>\n",
       "      <td>again no hot water  ( in the 35 )  moderate...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>6196673209d8f566a8033e74</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I say yes...</td>\n",
       "      <td>again no hot water  ( in the 35 )  moderate...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>6196673209d8f566a8033e74</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>With us also similar</td>\n",
       "      <td>again no hot water  ( in the 35 )  moderate...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>6196673209d8f566a8033e74</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>With me also</td>\n",
       "      <td>again no hot water  ( in the 35 )  moderate...</td>\n",
       "      <td>No Location</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6995 rows × 9 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-420671ed-220b-4ae5-984e-976ae2a9d236')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-420671ed-220b-4ae5-984e-976ae2a9d236 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-420671ed-220b-4ae5-984e-976ae2a9d236');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                       ID_POST        DATE  nb_comment  nb_post  nb_view  \\\n",
       "0     55002b8b828463a4178b457b  2015-03-11         1.0      1.0      0.0   \n",
       "1     5508260e828463ae4a8b45fa  2015-03-17         0.0      1.0      0.0   \n",
       "2     552547e8828463e9288b4586  2015-04-08         1.0      1.0      0.0   \n",
       "3     552f62c78284636b568b4577  2015-04-16         1.0      1.0      0.0   \n",
       "4     552fa7218284634b608b4581  2015-04-16         3.0      1.0      0.0   \n",
       "...                        ...         ...         ...      ...      ...   \n",
       "7769  6196673209d8f566a8033e74  2021-11-18         5.0      1.0      5.0   \n",
       "7770  6196673209d8f566a8033e74  2021-11-18         5.0      1.0      5.0   \n",
       "7771  6196673209d8f566a8033e74  2021-11-18         5.0      1.0      5.0   \n",
       "7772  6196673209d8f566a8033e74  2021-11-18         5.0      1.0      5.0   \n",
       "7773  6196673209d8f566a8033e74  2021-11-18         5.0      1.0      5.0   \n",
       "\n",
       "      nb_like                                            comment  \\\n",
       "0         0.0    Hello, urgent service messages do not arrive...   \n",
       "1         0.0                                         No Comment   \n",
       "2         0.0         Maybe take a look at the old apartment.      \n",
       "3         0.0    Good and original thing :-) Great and useful...   \n",
       "4         0.0    Thank you very much for this hint! We will b...   \n",
       "...       ...                                                ...   \n",
       "7769      0.0    With us exactly the same, we had to take a c...   \n",
       "7770      0.0    Yes  OK again, is already tedious? And unfor...   \n",
       "7771      0.0                                    I say yes...      \n",
       "7772      0.0                            With us also similar      \n",
       "7773      0.0                                       With me also   \n",
       "\n",
       "                                                   post        Location  \n",
       "0     The  team is the contact point for all questio...     No Location  \n",
       "1       How is a good neighbourhood created? Send th...     No Location  \n",
       "2       Dear , :  our cat is missing. The cat's name...     No Location  \n",
       "3        we want to know! Your opinion!  in the Erle...  Erlenmatt West  \n",
       "4       We from the  have the Erlenmattpark right in...     No Location  \n",
       "...                                                 ...             ...  \n",
       "7769     again no hot water  ( in the 35 )  moderate...     No Location  \n",
       "7770     again no hot water  ( in the 35 )  moderate...     No Location  \n",
       "7771     again no hot water  ( in the 35 )  moderate...     No Location  \n",
       "7772     again no hot water  ( in the 35 )  moderate...     No Location  \n",
       "7773     again no hot water  ( in the 35 )  moderate...     No Location  \n",
       "\n",
       "[6995 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h1>Conclusion Notebook 1</h1>\n",
    "    <p> Maintenant que nous avons créé un dataset avec toutes les informations nécessaires, nous pouvons exploiter les données, en tirer une information pertinente et créer notre webapp.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook 1 _DataCleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
